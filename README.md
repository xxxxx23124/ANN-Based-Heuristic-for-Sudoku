# RL-Sudoku-Solver：基于强化学习的数独求解器

本项目旨在使用强化学习（RL）训练一个能够自主学习并解决数独问题的智能体。

## 项目简介

传统的数独求解器通常依赖于固定的算法或监督学习。本项目采用强化学习，将数独求解视为一个序贯决策过程。这种方法的优势在于，智能体通过与环境（数独棋盘）的交互来学习一个通用的**求解策略**，而不是记忆特定问题的**固定答案**，因此在面对多解或无解的复杂情况时具有更好的泛化能力。

## 技术方案

-   **核心思想**: 智能体（Agent）在数独棋盘（Environment）上通过放置数字（Action）来与环境交互。每一步操作后，环境会根据规则（如数字是否合法、是否完成）给予奖励（Reward），智能体根据奖励不断优化其决策策略（Policy）。
-   **模型架构**: 采用基于 Mamba 和 Self-Attention 的神经网络架构，用于高效地处理和理解棋盘状态。
-   **训练算法**: 当前主要使用 PPO (Proximal Policy Optimization) 算法。

## 当前状态与挑战

### PPO 训练中的浮点精度误差问题

PPO 是一种**同策略（On-Policy）**算法，它要求训练和推理时使用完全相同的策略。然而，在项目实践中发现，由于浮点数计算的固有误差，导致两者策略产生了较大偏差（`approx_kl` 指标过高），这给训练带来了挑战。

误差主要来源于两个方面：
1.  **数据塑形 (Reshape)**: 在处理数据维度 `b, s, l, d` -> `(b*s), l, d` 时，s为1和不为1对于计算结果的误差影响较大。
2.  **Mamba2 模型机制**: Mamba2 模型在处理时间步 `s` 时，并行计算（训练时）和串行计算（推理时）的内在机制不同，进一步放大了这种误差。

### 关键发现与解释

**一个关键的发现是**：尽管初始策略偏差（`approx_kl`）很大，但经过一段时间的训练，`approx_kl` 可以降低到可接受的范围（如 0.02 以下），使得模型可以继续训练。

**原因推测**：
训练初期，模型的策略网络权重是随机的，输出的策略分布接近于均匀分布（高熵状态）。在这种不确定的状态下，任何微小的浮点计算误差都可能被放大，导致策略分布产生巨大差异。

随着训练的进行，模型逐渐学习到有效的走法，其输出的策略变得更加**确定和自信**（例如，在某个空格填“5”的概率远高于其他数字）。当策略分布变得“尖锐”（低熵状态）时，原先的浮点误差虽然仍然存在，但已不足以改变最终的决策结果或策略的整体形态，因此`approx_kl`（策略相似度）会显著下降，趋于稳定。

## 未来计划

为了从根本上解决同策略算法的限制，后续计划是开发基于 **离散 SAC (Soft Actor-Critic)** 的版本。SAC 是一种**异策略（Off-Policy）**算法，它允许使用历史数据进行训练，对训练和推理时策略的不一致性有更强的鲁棒性。

---

### 附录：维度解释

-   `b` (batch_size): 一批处理的棋盘数量。
-   `s` (sequence_length): 时间步长，或可理解为决策序列的长度。
-   `l` (length): 棋盘展平后的一维尺寸 (例如 9x9 -> 81)。
-   `d` (dimension): 模型内部特征的维度。